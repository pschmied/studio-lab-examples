{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cc364b-59b9-42e8-9cb6-431f6cfc9bb8",
   "metadata": {},
   "source": [
    "# Sample Julia Notebook - downloading and using data from S3\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/use-julia-in-studio-lab/2-sample-julia-nb.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e896402-784b-4566-8c14-e7bc8b615007",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "\n",
    "As we did in the Installing Julia notebook, we use the Pkg.add command to add a few packages that we will use for this notebook.\n",
    "\n",
    " - [Gadfly](http://gadflyjl.org/stable/): grammar of graphics plotting, similar to ggplot in R\n",
    " - [AWS](https://github.com/JuliaCloud/AWS.jl): third-party SDK for performing tasks in AWS\n",
    " - [AWSS3](https://github.com/JuliaCloud/AWSS3.jl): high-level third-party interface to Amazon Simple Storage Service\n",
    " - [DataFrames](https://dataframes.juliadata.org/stable/): data structure for working with tabular data, similar to Pandas in Python\n",
    " - [Query](http://www.queryverse.org/Query.jl/stable/): convenience functions for working with DataFrames and other queryable data sources, similar to dplyr in R.\n",
    " - [JSON](https://github.com/JuliaIO/JSON.jl): library for parsing JSON data\n",
    " - [TranscodingStreams](https://juliaio.github.io/TranscodingStreams.jl/latest/): framework for working with encoded data\n",
    " - [CodecZlib](https://github.com/JuliaIO/CodecZlib.jl): zlib compression codec for use with TranscodingStreams; allows us to work with gzipped data.\n",
    " \n",
    "After installing the packages, we'll load them as we use them in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4215fe4-00cb-41d1-b7f7-1f03f247847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"Gadfly\", \"AWS\", \"AWSS3\", \"DataFrames\", \"Query\", \"JSON\", \"TranscodingStreams\", \"CodecZlib\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03831149-6b10-457b-bf8e-34fad25944b6",
   "metadata": {},
   "source": [
    "## Example plots using Gadfly\n",
    "\n",
    "First up, we'll plot a histogram of some normally-distributed random numbers. Then, we'll plot an analogous 2 dimensional density plot of some normally-distributed random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a5b21-2859-46da-a70e-7a649e581a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "\n",
    "# Note that Gadfly is JIT compiled, which means that it can take a little time\n",
    "# for the first plot to render. Subsequent plots are speedy.\n",
    "plot(x = randn(10000),\n",
    "     Geom.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941ab84-b2ab-4256-8018-32259fa37e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x = randn(1000),\n",
    "     y = randn(1000),\n",
    "     Geom.density2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebe5dc-5d71-4e09-93ae-a29d718da4f9",
   "metadata": {},
   "source": [
    "## Accessing and using data from Amazon S3\n",
    "\n",
    "Here we will use the third-party AWS SDK and high-level S3 interface. The data we will use [are listed at the Registry of Open Data on AWS (RODA)](https://registry.opendata.aws/lei/), and are stored in a public S3 bucket s3://gleif/ in the eu-central-1 region. This dataset uniquely identifies legal entities using a code, and also provides information about ownership structures.\n",
    "\n",
    "We will access these data anonymously, which can be done by creating and using an `aws_config` structure with `creds=nothing`. Note that we must also supply the bucket's region with our configuration.\n",
    "\n",
    "Our first operation will be to list out all the keys in this S3 bucket that begin with \"data/json/lei\". We can see that the data are organized by date and time and have a \".json.gz\" extension, denoting that these data are json formatted and compressed using gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbc83b-7241-4bc6-a6f0-18cb98d5097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using AWS\n",
    "using AWSS3 # high-level interface to S3\n",
    "\n",
    "# This AWSConfig instance allows us to make anonymous / unsigned requests\n",
    "aws = global_aws_config(;creds = nothing, region = \"eu-central-1\")\n",
    "\n",
    "# s3_list_keys returns a generator that we can collect immediately using collect\n",
    "obj = collect(s3_list_keys(aws, \"gleif\", \"data/json/lei/\"; delimiter = \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2d5c3-08ba-48fc-b0df-e1dee7aeda16",
   "metadata": {},
   "source": [
    "Next, we'll load one of the compressed JSON objects from S3. Since it is gzipped, we'll need to decompress the object. And because the object is also in JSON format, we'll need to parse the JSON to work with it in Julia.\n",
    "\n",
    "Objects in S3 can be very large ([up to 5TB in a single object as of this writing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html)). Processing larger objects may require an incremental strategy, however in this case the objects we're interested in can safely fit in memory. We therefore download, decompress, and parse the whole object at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca693-52c3-4f40-816b-08f6954876e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JSON, TranscodingStreams, CodecZlib\n",
    "\n",
    "rr = JSON.parse(String(transcode(GzipDecompressor, read(S3Path(\"s3://gleif/data/json/rr/date=2021-12-10/time=00:00/20211210-0000-gleif-goldencopy-rr.json.gz\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143be21e-32b1-4629-91f9-66992e079d98",
   "metadata": {},
   "source": [
    "The resulting dictionary object contains a \"relations\" key that contains a number of records pertaining to the relationships between legal entities. To help us understand the structure of an individual record, we can print the first record as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1ba1e-b272-49d1-ace6-d6a977498fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json(rr[\"relations\"][1], 4)) # the 4 tells, the json library to indent using 4 spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644deb47-de5d-4d28-bc58-b1b44682bfff",
   "metadata": {},
   "source": [
    "To help us do some simple analysis of these relationship records, we'll construct a DataFrame containing just a handful fields that we're interested in. Here we construct the DataFrame from named columns of vectors extracted from the dictionary we generated from the JSON from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d90a1c-c2c1-4770-a5fe-3abe9538dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, Query\n",
    "\n",
    "relations = DataFrame(\n",
    "    startnode = map(x -> x[\"RelationshipRecord\"][\"Relationship\"][\"StartNode\"][\"NodeID\"][\"\\$\"], rr[\"relations\"]),\n",
    "    endnode = map(x -> x[\"RelationshipRecord\"][\"Relationship\"][\"EndNode\"][\"NodeID\"][\"\\$\"], rr[\"relations\"]),\n",
    "    relationshiptype = map(x -> x[\"RelationshipRecord\"][\"Relationship\"][\"RelationshipType\"][\"\\$\"], rr[\"relations\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d1f6f-6ccd-4099-8854-5a6376dc1426",
   "metadata": {},
   "source": [
    "Once data are loaded into a DataFrame object, it is straightforward to perform various table operations. Here we use macros from the Query package to refine our DataFrame as a pipeline. Here we limit the results to rows where the relationshiptype is \"IS_DIRECTLY_CONSOLIDATED_BY\", then group by values in the endnode column, then get a count of records per group, and lastly sort the table in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dac48e-56d4-4435-9885-c8330767a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = relations |>\n",
    "    @filter(_.relationshiptype == \"IS_DIRECTLY_CONSOLIDATED_BY\") |>\n",
    "    @groupby(_.endnode) |>\n",
    "    @map({endnode=key(_), count = length(_)}) |>\n",
    "    @orderby_descending(_.count) |>\n",
    "    DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efab50-36a4-4cc9-b48f-6ff70644f2fc",
   "metadata": {},
   "source": [
    "If we wanted to join names for all these groups, we could extract that information from the LEI data similarly housed on S3. Or if we are just casually interested in the top item, we can also [search for a LEI record on GLEIF's website](https://search.gleif.org/#/search/simpleSearch=784F5XWPLTWKTBV3E584&fulltextFilterId=LEIREC_FULLTEXT&currentPage=1&perPage=15&expertMode=false#search-form). In this case, the largest direct consolidator is The Goldman Sachs Group, Inc.\n",
    "\n",
    "Lastly, we can plot the distribution of group sizes, once again using the Gadfly plotting library. Here we see that most direct consolidators have far fewer entries—one or two—and that there is a very long tail of larger direct consolidators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6469a0-a78e-4ff2-9f05-3abc1c9e9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(group_sizes, x=:count, Geom.density)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Julia (4 threads) 1.7.0",
   "language": "julia",
   "name": "conda-env-default-julia-_4-threads_-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
